[WIKI](https://en.wikipedia.org/wiki/Correlation)

> Самое главное в корреляции – то, что это мера наблюдаемой связи, сама по себе она никак не может выявить причину. Многие переменные в реальном мире сильно коррелируют друг с другом, но эти связи могут объясняться случаем, влиянием других переменных или другими неизвестными причинами. Даже если между величинами есть причинно-следственная связь, она может работать в другую сторону, чем мы предполагаем. Поэтому даже самая сильная корреляция сама по себе не может свидетельствовать о причинно-следственной связи; она может быть подтверждена только с помощью постановки эксперимента [[0. Books/Статистика для всех, Бослаф С./0. cover]]

$$ r = \frac{SS_{xy}}{\sqrt{SS_x*SS_y}} $$
$SS_x$ – это сумма квадратов отклонений x $SS_x=\sum_{i=1}^{n}{(x_i - \bar x)^2}$
$SS_y$ – это сумма квадратов отклонений y $SS_y=\sum_{i=1}^{n}{(y_i - \bar y)^2}$
$SS_{xy}$ – это сумма квадратов отклонений x и y. $SS_{xy}=\sum_{i=1}^{n}{(x_i - \bar x)(y_i - \bar y)}$

>Другие оценки корреляции В статистике давно были предложены другие типы коэффициентов корреляции, такие как коэффициент ранговой корреляции Спирмена ρ (ро) или коэффициент ранговой корреляции Кендалла τ (тау). Эти коэффициенты корреляции основаны на ранге данных, т. е. номерах наблюдений в наборе. Поскольку они работают с рангами, а не со значениями, эти оценки устойчивы к выбросам и могут справляться с определенными типами нелинейности. Однако аналитики данных в целях разведочного анализа могут обычно придерживаться коэффициента корреляции Пирсона и его робастных альтернатив. Ранговые оценки привлекательны главным образом в случае небольших наборов данных и определенных проверок статистических гипотез. [[0. Books/Практическая статистика для специалистов Data Science/0. cover]]
